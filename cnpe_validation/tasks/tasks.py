import json
import zipfile
from abc import abstractmethod
from functools import partial
from io import BytesIO

import equinox as eqx
import jax.numpy as jnp
import requests
from cnpe.models import AbstractNumpyroGuide, AbstractNumpyroModel
from cnpe.numpyro_utils import (
    get_sample_site_names,
    shape_only_trace,
    validate_data_and_model_match,
)
from jaxtyping import Array, PRNGKeyArray
from numpyro.infer import Predictive
from numpyro.util import check_model_guide_match


class AbstractTask(eqx.Module):
    """A model, guide and method for generating ground truth samples."""

    model: eqx.AbstractVar[AbstractNumpyroModel]
    guide: eqx.AbstractVar[AbstractNumpyroGuide]
    name: eqx.AbstractClassVar[str]

    def __check_init__(self):
        model_trace = shape_only_trace(self.model)
        obs = {}
        for name in self.model.observed_names:
            if name not in model_trace:
                raise ValueError(
                    f"Trace of model does not include observed node {name}.",
                )
            obs[name] = jnp.empty(
                shape=model_trace[name]["value"].shape,
                dtype=model_trace[name]["value"].dtype,
            )  # keep runtime type checker happy

        check_model_guide_match(
            model_trace=shape_only_trace(self.model, obs=obs),
            guide_trace=shape_only_trace(self.guide, obs=obs),
        )

    def get_observed_and_latents_and_check(self, key: PRNGKeyArray):
        """get_observed_and_latents and checks matches model trace and model.observed_names."""
        obs, latents = self.get_observed_and_latents(key)

        # These should be methods defined on abstract class?
        validate_data_fn = partial(
            validate_data_and_model_match,
            model=self.model.call_without_reparam,
        )

        if isinstance(self, AbstractTaskWithReference):
            validate_data_fn(obs)
            eqx.filter_vmap(validate_data_fn)(latents)  # Batch of reference samples
        else:
            validate_data_fn(obs | latents)

        data = {"observed": obs, "latents": latents}
        for key, dat in data.items():
            for name, arr in dat.items():
                dat[name] = eqx.error_if(
                    arr,
                    ~jnp.isfinite(arr),
                    f"{name} in {key} had non-finite values",
                )

        return data["observed"], data["latents"]

    def _check_data_names(self, obs: dict[str, Array], latents: dict[str, Array]):
        latent_names = (
            get_sample_site_names(self.model.call_without_reparam).all
            - self.model.observed_names
        )

        if obs.keys() != self.model.observed_names:
            raise ValueError(
                f"Obsersvations had keys {obs.keys()}, but model had "
                f"{self.model.observed_names}",
            )
        if latents.keys() != latent_names:
            raise ValueError(
                f"Latents had keys {latents.keys()}, but model had {latent_names}.",
            )

    @abstractmethod
    def get_observed_and_latents(
        self,
        key: PRNGKeyArray,
    ) -> tuple[dict[str, Array], dict[str, Array]]:
        """Get the observations and parameters.

        The parameters are from a reference posterior if available, otherwise, they are
        the ground truth parameters used to generate the observation.
        """

    @abstractmethod
    def validate_data(self, obs: dict[str, Array], latents: dict[str, Array]):
        """Checks the data produced by get_observed_and_latents is what is expected.

        Specifically, checks the shapes and names are correct, and that a batch
        dimension in the latents is present for tasks with a reference posterior.
        """
        pass


class AbstractTaskWithoutReference(AbstractTask):
    """A task without a reference posterior.

    The observation and parameters are generated by sampling the model.
    """

    def get_observed_and_latents(self, key):
        """Generate an observation and ground truth latents from the model."""
        latents = Predictive(self.model.call_without_reparam, num_samples=1)(key)
        latents = {k: v.squeeze(0) for k, v in latents.items()}
        obs = {name: latents.pop(name) for name in self.model.observed_names}
        return obs, latents

    def validate_data(self, obs: dict[str, Array], latents: dict[str, Array]):
        self._check_data_names(obs, latents)
        validate_data_and_model_match(obs | latents, self.model.call_without_reparam)


class AbstractTaskWithReference(AbstractTask):
    """A task with a corresponding reference posterior."""

    def validate_data(self, obs: dict[str, Array], latents: dict[str, Array]):
        """Checks that there exists a single batch dimension in latents."""
        self._check_data_names(obs, latents)
        validation_fn = partial(validate_data_and_model_match)(
            model=self.model.call_without_reparam,
        )
        validation_fn(obs)
        eqx.filter_vmap(validation_fn)(latents)


# TODO Consider using associated python package
def get_posterior_db_reference_posterior(name) -> dict:
    """Get the reference posterior draws from posteriordb.

    https://github.com/stan-dev/posteriordb

    Args:
        name: The name of the zip file containing the draws, exluding the extension.
    """
    # Targetting tagged release 0.5.0 for better reproducibility
    url = f"https://github.com/stan-dev/posteriordb/raw/0.5.0/posterior_database/reference_posteriors/draws/draws/{name}.json.zip"

    # Send a GET request to the URL
    response = requests.get(url)
    response.raise_for_status()
    zip_content = BytesIO(response.content)

    # Extract the zip file
    with zipfile.ZipFile(zip_content, "r") as zip_ref:
        assert len(zip_ref.infolist()) == 1
        zip_info = zip_ref.infolist()[0]

        # Extract the JSON file from the zip
        with zip_ref.open(zip_info) as json_file:
            # Read the JSON data
            draws = json.load(json_file)

    # Concatenate the chains
    draws = {
        k: jnp.concatenate([jnp.asarray(chain[k]) for chain in draws])
        for k in draws[0].keys()
    }

    # Names may be of form param[1] param[2]. We want to stack these into an array
    stacked_draws = {k.split("[")[0]: [] for k in draws.keys()}
    for k, v in draws.items():
        key_root = k.split("[")[0]
        stacked_draws[key_root].append(v)

    return {k: jnp.stack(v, axis=-1).squeeze() for k, v in stacked_draws.items()}
